# **План випускного проєкту**

**Призначення документа:** Структурування ідеї, планування технічних кроків та підготовка основи для фінального захисту (презентації).

## **1\. Загальна інформація**

| Поле | Заповнення |
| ----- | ----- |
| **Профіль навчання** | Основи інженерії штучного інтелекту |
| **Тема проєкту (Коротка назва)** | SignSpeak: AI-перекладач жестової мови в текст і голос |
| **Виконавець** | Бугай Олександр Олексійович. 11-г клас |
|  |  |
|  |  |

## **2\. Обґрунтування та Цілі Проєкту**

### **2.1. Актуальність та Проблема**

Люди з порушеннями слуху щодня стикаються з комунікаційними бар’єрами: не в усіх ситуаціях є сурдоперекладач, не всі володіють жестовою мовою, а онлайн-сервіси рідко підтримують її автоматичний переклад. Це обмежує доступ до інформації, освіти та сервісів для десятків тисяч людей.

Мій проєкт спрямований на створення прототипу системи, яка за допомогою вебкамери розпізнає жести користувача і перетворює їх на текст та озвучений голос. Такий інструмент може стати основою для майбутніх інклюзивних рішень у сфері освіти, сервісів та спілкування в реальному часі.

Проєкт поєднує комп’ютерний зір, машинне навчання та обробку послідовностей і демонструє практичне застосування штучного інтелекту для вирішення важливої соціальної проблеми — доступної комунікації.

### **2.2. Мета Проєкту (Кінцевий результат)**

Створити застосунок на основі Python, OpenCV, MediaPipe та моделі машинного навчання для розпізнавання жестової мови з вебкамери і автоматичного перекладу жестів у текст та синтезований голос українською мовою в режимі, наближеному до реального часу.

### **2.3. Завдання проєкту (Кроки)**

1. **Вибрати та затвердити ідею проєкту**  
    Описати, що саме я роблю: перекладач жестової мови з камери в текст і голос.  
2. **Налаштувати середовище розробки**  
    Встановити Python, OpenCV, MediaPipe та потрібні бібліотеки.  
3. **Зібрати дані для навчання**  
   1. записати/зібрати приклади жестів (літери, кілька слів/фраз);  
      підготувати ці дані у зручному форматі (папки/таблиці).  
4. **Зробити модуль, який “бачить” руку**  
    Підключити камеру, через MediaPipe отримувати координати точки на руці.  
5. **Навчити модель розпізнавати жести**  
   1. для статичних жестів (літери) – модель класифікації;  
   2. для кількох слів/фраз – простий алгоритм для послідовностей жестів.  
6. **Зробити переклад у текст та (за бажанням) голос**  
   1. показувати розпізнаний жест як текст на екрані;  
   2. додати озвучку розпізнаних слів/фраз.  
7. **Створити простий інтерфейс програми**  
    Вікно з відео з камери, накресленою рукою та полем з текстом перекладу.  
8. **Протестувати та покращити систему**  
    Перевірити точність на різних жестах, виправити помилки, оптимізувати швидкість.  
9. **Підготувати захист**  
    Зробити презентацію, продумати сценарій демонстрації та відповіді на можливі запитання.

## **3\. Технічний план реалізації**

### **3.1. Ключові Технології та Інструменти**

Перерахуйте все, що буде використано.

| Категорія | Обрані технології | Обґрунтування вибору |
| ----- | ----- | ----- |
| **Мова програмування** | Python | Стандарт для задач ШІ та комп’ютерного зору, велика кількість бібліотек і прикладів. |
| **Фреймворки / API** | OpenCV, MediaPipe API, [PySimpleGUI](https://github.com/PySimpleGUI/PySimpleGUI/blob/master/DemoPrograms/Demo_OpenCV.py) |  для захоплення відео з камери, обробки кадрів і відображення результатів; PySimpleUI \- для віконного інтерфейсу з підтримкою OpenCV |
| **Ключові бібліотеки/Моделі** | MediaPipe Hand Landmarker https://mediapipe-studio.webapps.google.com/demo/hand\_landmarker | MediaPipe \- готова модель що знаходить ключові точки руки |
| **Датасет** | Набір фото з жестами у дереві каталогів | Не потрібна база даних, тому що набір фото опрацьовується один раз при тренуванні моделі |

### **3.2. Алгоритм роботи системи (Покроковий опис)**

**Вхід: Користувач стає у поле зору вебкамери та показує жест (літеру, слово або коротку фразу).**

**Захоплення відео: Застосунок за допомогою OpenCV отримує потік відео з камери та обробляє його покадрово.**

**Виділення ознак (MediaPipe): Кожен кадр передається в модуль MediaPipe, який знаходить руку/руки та повертає координати ключових точок (landmarks) пальців та долоні.**

| Ім’я файлу | Точка 1 X | Точка 1 Y | Точка 2 X | Точка 2 Y | … | Жест |
| :---- | :---- | :---- | :---- | :---- | :---- | :---- |
| **image1.jpg** | **15** | **21** | **44** | **21** |  | **Привіт** |
| **image2.jpg** | **15** | **21** | **44** | **21** |  | **Привіт** |
| **image3.jpg** | **15** | **21** | **44** | **21** |  | **До побачення** |
| **image4.jpg** | **15** | **21** | **44** | **21** |  | **До побачення** |

**\-\> на цій таблиці тренуємо модель**

* **формуються вектор ознак для статичного жесту**   
* **Розпізнавання жесту:**  
* **для статичних поз (алфавіт) вектор ознак подається на модель класифікації (Random Forest/MLP), яка повертає передбачену літеру;**  
    
  **Модель розпізнає жест із даних:**

| Ім’я файлу | Точка 1 X | Точка 1 Y | Точка 2 X | Точка 2 Y | … |
| :---- | :---- | :---- | :---- | :---- | :---- |
| **нова картинка** | **15** | **21** | **44** | **21** |  |

* **\-\> запускається модель та видає результат класифікації: Привіт чи До побачення (чи щось інше)**  
    
  **Формування тексту:**  
* **якщо розпізнано окрему літеру — вона додається до поточного текстового буфера;**

* **якщо розпізнано слово/фразу — додається як окреме слово (з пробілом) до текстового рядка.**  
  **Синтез мовлення (опційно): Розпізнане слово/фразу передаються в модуль TTS (pyttsx3), який озвучує текст голосом.**  
  **Вихід:**  
* **на екрані відображається відео з камери з накладеним “скелетом” руки для наочності;**  
* **поруч виводиться текст перекладу;**  
* **(опційно) відтворюється звук з озвученим перекладом.**  
1. 

### **3.3. Макет**

![][image1]

## **4\. Очікувані результати**

### **4.1. Критерії успіху (Що вважається завершеним проєктом?)**

Застосунок стабільно працює з вебкамерою на цільовому комп’ютері, не падає з помилками під час демонстрації.

Система коректно розпізнає не менше 10–15 статичних жестів (наприклад, частину алфавіту та кілька службових жестів) з точністю не нижче \~85% на тестових прикладах.

Затримка між показом жесту і появою тексту не перевищує \~0,5–2 секунди для статичних жестів (режим, наближений до реального часу).  
Реалізована можливість автоматичного озвучування розпізнаних слів/фраз.

Підготовлено презентацію з описом архітектури, результатами тестування (приблизна точність, приклади вдалих і помилкових розпізнавань) та проведена жива демонстрація проєкту.

### **4.2. Аналіз критеріїв захисту (Допомога для фінальної презентації)**

**Відповідність темі – 5%**  
 На початку виступу чітко сформулюю проблему: комунікаційні бар’єри для людей з порушеннями слуху та відсутність доступного автоматичного перекладу жестової мови. Поясню, що проєкт є саме інженерним рішенням у сфері ШІ і спрямований на інклюзію.

**Технічна реалізація – 35%**  
 Покажу блок-схему архітектури: вебкамера → MediaPipe → модель класифікації → текст → TTS.  
 Продемонструю код ключових модулів , поясню, чому обрав саме ці технології . Запущу застосунок у реальному часі і покажу, як він розпізнає кілька жестів.

**Креативність та оригінальність – 20%**  
 Підкреслю, що проєкт не є “черговим чат-ботом” чи “класифікатором котів/собак”, а комплексною системою, яка перекладає візуальну мову (жести рук) в текст і голос. Зазначу, що використовується комбінація сучасних підходів а також підтримка саме української мови/контексту. Розкажу про власний внесок у збір даних (запис власних жестів) та адаптацію під локальну специфіку. Порівняти з аналогами та зробити feature matrix.

**Використання даних – 10%**  
 Опишу, які датасети використовувалися (якщо брав публічні — вкажу джерела) та як я збирав власні дані (відеозаписи жестів). Поясню, як очищав та готував дані: нормалізація координат, баланс класів, поділ на навчальну та тестову вибірки. Наголошу, що дотримувався академічної доброчесності: вказав всі джерела даних і чужий код (якщо щось використовував як референс), а модель і логіку системи реалізував самостійно.

**Презентація – 30%**  
 Підготую чітку, візуально чисту презентацію з мінімумом тексту та зрозумілими схемами.  
 Структура: проблема → ідея → архітектура → демонстрація → результати → обмеження → перспективи розвитку.  
 Проведу живу демонстрацію: покажу кілька жестів перед камерою, дам системі перекласти їх у текст і голос. Відповім на питання комісії, зокрема про вибір технологій, обмеження моделі і можливі майбутні покращення.

